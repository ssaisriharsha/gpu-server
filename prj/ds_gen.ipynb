{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae398211-8b7c-4838-894f-da00021e6b20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.2.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in ./lib/python3.12/site-packages (from sentence-transformers) (4.57.6)\n",
      "Requirement already satisfied: tqdm in ./lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in ./lib/python3.12/site-packages (from sentence-transformers) (2.10.0)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Downloading scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Downloading scipy-1.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m816.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.20.0 in ./lib/python3.12/site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in ./lib/python3.12/site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in ./lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: requests in ./lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: setuptools in ./lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (80.10.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in ./lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: cuda-bindings==12.9.4 in ./lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.9.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in ./lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in ./lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.6.0 in ./lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: cuda-pathfinder~=1.1 in ./lib/python3.12/site-packages (from cuda-bindings==12.9.4->torch>=1.11.0->sentence-transformers) (1.3.3)\n",
      "Requirement already satisfied: numpy>=1.17 in ./lib/python3.12/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./lib/python3.12/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2026.1.15)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./lib/python3.12/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./lib/python3.12/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Collecting joblib>=1.3.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting threadpoolctl>=3.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2026.1.4)\n",
      "Downloading sentence_transformers-5.2.0-py3-none-any.whl (493 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading scipy-1.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (35.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.0/35.0 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.1/309.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn, sentence-transformers\n",
      "Successfully installed joblib-1.5.3 scikit-learn-1.8.0 scipy-1.17.0 sentence-transformers-5.2.0 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1ee6a4d-b8fb-4031-b8b0-0e34e112d9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.nn.functional import softmax\n",
    "from sentence_transformers import CrossEncoder\n",
    "from tqdm.notebook import tqdm\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "399203a3-d930-47fc-b991-acf20a2d3825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7c1ea29cef90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_checkpoint=\"s-nlp/roberta_toxicity_classifier\"\n",
    "paraphraser_checkpoint=\"dphn/dolphin-2.9.3-mistral-nemo-12b\"\n",
    "similarity_checkpoint=\"cross-encoder/stsb-roberta-large\"\n",
    "dataset=\"SetFit/toxic_conversations\"\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1acc9b6-155a-4392-a550-0fa292c7a381",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at s-nlp/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73460fc360da43d98b826273c956fd5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier=AutoModelForSequenceClassification.from_pretrained(classifier_checkpoint, dtype=torch.bfloat16, device_map=\"auto\")\n",
    "paraphraser=AutoModelForCausalLM.from_pretrained(paraphraser_checkpoint, dtype=torch.bfloat16, device_map=\"auto\")\n",
    "similarity=CrossEncoder(similarity_checkpoint, model_kwargs={\"dtype\": torch.bfloat16})\n",
    "classifier_tokenizer=AutoTokenizer.from_pretrained(classifier_checkpoint)\n",
    "paraphraser_tokenizer=AutoTokenizer.from_pretrained(paraphraser_checkpoint, fix_mistral_regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b574380e-362a-4c2c-8df2-a258a44ed2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 cuda:0 cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(classifier.device, paraphraser.device, similarity.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b658777-43d2-4146-883f-8df69ff3545a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff68efdbbd3640879ec152ea86510b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'label_text'],\n",
       "    num_rows: 3954\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds=load_dataset(dataset, split=\"test\")\n",
    "ds=ds.filter(lambda ex: ex[\"label\"]==1)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e34060ba-d6db-4606-8d8b-285e8b4a084f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop making excuse for your infantile and stupid behavior.  there are ways to express your displeasure but showing your finger is not one of these ways.  you have disrespected the process and brought shame to the Hawaii delegation and people.   you owe the people of Hawaii an apology and to the DNC.  If you refuse to apologize then you should resign your position with the Hawaii democratic party.  there is no bravery involved just plain stupidity.   you are stooping low like trump is doing in his immature behavior when people disagree with him.   maybe you should join the repubs, you should feel at home with them.  dems do not tolerate stupidity.   no fd, she does not love Hawaii, she lovers herself, if she did she would not have done what she did to bring shame to us. 1\n"
     ]
    }
   ],
   "source": [
    "ex=ds[0]\n",
    "sentence=ex[\"text\"]\n",
    "label=ex[\"label\"]\n",
    "print(sentence, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdc82694-f3f3-430a-a254-396cc73118e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt=\"\"\"You are a linguistic transformation agent. Your task is to analyze an input sentence and rewrite it by shifting its tone and toxicity level while maintaining 100% semantic equivalence.\n",
    "Rules:\n",
    "1. Analysis: Identify the current tone (e.g., polite, neutral, aggressive) and toxicity level.\n",
    "2. Transformation: If the input is civil, rewrite it to be toxic/hostile. If the input is toxic, rewrite it to be civil/professional.\n",
    "3. Meaning: Do not change the core facts or the intent of the message.\n",
    "4. Output Format: Provide only the paraphrased sentence.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fec8bf21-4b28-4a63-b729-cafd62301332",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Your behaviour was unrefined and shallow. You should be able to communicate dissatisfaction in a different manner. Your gesture was inappropriate and it has defamed the reputation of the Hawaii delegation. Apologizing to the people of Hawaii and the DNC is what you ought to do. In case you don't want to apologize, you should step down from your position within the Hawaii Democratic party. There is no courage in this act, simply foolishness. Your actions are reflective of Trump, showing immaturity when faced with disagreement. Perhaps you should join the Republicans as you would probably be more comfortable there. The Democrats do not condone absurdity. She doesn't genuinely love Hawaii. If she did, she wouldn't have caused humiliation to us by her actions.\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    prompt=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": sentence}\n",
    "    ]\n",
    "    tokenized_prompt=paraphraser_tokenizer.apply_chat_template(prompt,\n",
    "                                                   add_generation_prompt=True,\n",
    "                                                   tokenizer=True,\n",
    "                                                   return_tensors=\"pt\").to(paraphraser.device)\n",
    "    inp_length=len(tokenized_prompt[0])\n",
    "    inp_length\n",
    "    paraphraser_tokenizer.decode(paraphraser.generate(tokenized_prompt, attention_mask=torch.ones_like(tokenized_prompt), max_new_tokens=256)[0][inp_length:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f052b38-d20b-450c-8e73-75bada94188b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_sentence(sentence, model, tokenizer):\n",
    "    prompt=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": system_prompt}\n",
    "    ]\n",
    "    tokenized_prompt=tokenizer.apply_chat_template(prompt,\n",
    "                                                   add_generation_prompt=True,\n",
    "                                                   tokenizer=True,\n",
    "                                                   return_tensors=\"pt\").to(model.device)\n",
    "    inp_length=len(tokenized_prompt[0])\n",
    "    attention_mask=torch.ones_like(tokenized_prompt)\n",
    "    op=model.generate(tokenized_prompt, attention_mask=attention_mask, max_new_tokens=256)[0][inp_length:]\n",
    "    return tokenizer.decode(op, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7888923c-9e98-4cde-bce4-79e27d1fe074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_toxicity(sentence, model, tokenizer):\n",
    "    tokenized_sentence=tokenizer(sentence, return_tensors=\"pt\").to(model.device)\n",
    "    logits=model(**tokenized_sentence).logits\n",
    "    preds=softmax(logits, dim=-1)\n",
    "    res=np.argmax(preds.half().cpu().detach().numpy(), axis=-1)\n",
    "    return int(res[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d13b6d6-f874-4b89-9b34-4f42d1cb3f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_similarity(old_sentence, new_sentence, model):\n",
    "    return float(model.predict([old_sentence, new_sentence]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad06a288-c9d9-4cb2-bdaf-a54234bd848c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_sample(ex,\n",
    "                        classifier,\n",
    "                        classifier_tokenizer,\n",
    "                        paraphraser,\n",
    "                        paraphraser_tokenizer,\n",
    "                        similarity_checker):\n",
    "    old_sentence=ex[\"text\"]\n",
    "    old_label=ex[\"label\"]\n",
    "    new_sentence=generate_new_sentence(old_sentence, paraphraser, paraphraser_tokenizer)\n",
    "    new_label=estimate_toxicity(old_sentence, classifier, classifier_tokenizer)\n",
    "    if new_label==old_label:\n",
    "        return None\n",
    "    similarity=check_similarity(old_sentence, new_sentence, similarity_checker)\n",
    "    if similarity<0.4:\n",
    "        return None\n",
    "    return {\n",
    "        \"en_toxic_comment\": old_sentence if old_label==1 else new_sentence,\n",
    "        \"en_neutral_comment\": new_sentence if old_label==1 else old_sentence,\n",
    "        \"sentence_similarity\": similarity\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ace3f403-c32e-4478-9bee-ebb6b1d73cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_samples(ds,\n",
    "                         classifier,\n",
    "                         classifier_tokenizer,\n",
    "                         paraphraser,\n",
    "                         paraphraser_tokenizer,\n",
    "                         similarity_checker):\n",
    "    new_ds=[]\n",
    "    for ex in tqdm(ds):\n",
    "        new_ex=generate_new_sample(ex,\n",
    "                                   classifier,\n",
    "                                   classifier_tokenizer,\n",
    "                                   paraphraser,\n",
    "                                   paraphraser_tokenizer,\n",
    "                                   similarity_checker)\n",
    "        if new_ex is None:\n",
    "            continue\n",
    "        new_ds.append(new_ex)\n",
    "    return new_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace7a3ed-ae9c-411e-b380-b10a7d4c4bf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ed44964fa2f42e39c4e24bbe13b2e37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3954 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "new_ds=generate_new_samples(ds,\n",
    "                            classifier,\n",
    "                            classifier_tokenizer,\n",
    "                            paraphraser,\n",
    "                            paraphraser_tokenizer,\n",
    "                            similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1eaa6450-065d-4d7a-b723-2029836c6b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8566d0a12e0c4b9094c2cc9486706cf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33255bd2-02e2-422a-815e-5fe4f3cad771",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds=pd.DataFrame(new_ds)\n",
    "new_ds=Dataset.from_pandas(new_ds, preserve_index=False)\n",
    "new_ds.push_to_hub(\"harsha2946/sampl-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "adb8eade-ed5b-483b-8f1c-4d5ef65c6532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mSignature:\u001b[39m\n",
       "Dataset.from_pandas(\n",
       "    df: pandas.DataFrame,\n",
       "    features: Optional[datasets.features.features.Features] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    info: Optional[datasets.info.DatasetInfo] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    split: Optional[datasets.splits.NamedSplit] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    preserve_index: Optional[bool] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       ") -> \u001b[33m'Dataset'\u001b[39m\n",
       "\u001b[31mDocstring:\u001b[39m\n",
       "Convert `pandas.DataFrame` to a `pyarrow.Table` to create a [`Dataset`].\n",
       "\n",
       "The column types in the resulting Arrow Table are inferred from the dtypes of the `pandas.Series` in the\n",
       "DataFrame. In the case of non-object Series, the NumPy dtype is translated to its Arrow equivalent. In the\n",
       "case of `object`, we need to guess the datatype by looking at the Python objects in this Series.\n",
       "\n",
       "Be aware that Series of the `object` dtype don't carry enough information to always lead to a meaningful Arrow\n",
       "type. In the case that we cannot infer a type, e.g. because the DataFrame is of length 0 or the Series only\n",
       "contains `None/nan` objects, the type is set to `null`. This behavior can be avoided by constructing explicit\n",
       "features and passing it to this function.\n",
       "\n",
       "Important: a dataset created with from_pandas() lives in memory\n",
       "and therefore doesn't have an associated cache directory.\n",
       "This may change in the future, but in the meantime if you\n",
       "want to reduce memory usage you should write it back on disk\n",
       "and reload using e.g. save_to_disk / load_from_disk.\n",
       "\n",
       "Args:\n",
       "    df (`pandas.DataFrame`):\n",
       "        Dataframe that contains the dataset.\n",
       "    features ([`Features`], *optional*):\n",
       "        Dataset features.\n",
       "    info (`DatasetInfo`, *optional*):\n",
       "        Dataset information, like description, citation, etc.\n",
       "    split (`NamedSplit`, *optional*):\n",
       "        Name of the dataset split.\n",
       "    preserve_index (`bool`, *optional*):\n",
       "        Whether to store the index as an additional column in the resulting Dataset.\n",
       "        The default of `None` will store the index as a column, except for `RangeIndex` which is stored as metadata only.\n",
       "        Use `preserve_index=True` to force it to be stored as a column.\n",
       "\n",
       "Returns:\n",
       "    [`Dataset`]\n",
       "\n",
       "Example:\n",
       "\n",
       "```py\n",
       ">>> ds = Dataset.from_pandas(df)\n",
       "```\n",
       "\u001b[31mFile:\u001b[39m      /workspace/ha/prj/lib/python3.12/site-packages/datasets/arrow_dataset.py\n",
       "\u001b[31mType:\u001b[39m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c40d8ad-954f-4041-8a8e-8f6b81f6d5e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
